{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "c1d9537c-0dc0-4788-a0e5-0f15ba135eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os,glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fafc63-35a9-4e8e-adb8-89829ccd898f",
   "metadata": {},
   "source": [
    "# Metrics  \n",
    "The metrics are based on true positives (TP) and false positives (FP) determined not only by correct or wrong detections, but also based on if they are closer or further than a distance threshold T∘ (angular in our case) from the reference. For the evaluation of this challenge we take this threshold to be T=20∘.  \n",
    "  \n",
    "More specifically, for each class c∈[1,...,C] and each frame or segment:  \n",
    "  \n",
    "Pc predicted events of class c are associated with Rc reference events of class c  \n",
    "false negatives are counted for misses: FNc=max(0,Rc−Pc)  \n",
    "false positives are counted for extraneous predictions: FPc,∞=max(0,Pc−Rc)  \n",
    "Kc predictions are spatially associated with references based on Hungarian algorithm: Kc=min(Pc,Rc). Those can also be considered as the unthresholded true positives TPc=Kc.  \n",
    "the spatial threshold is applied which moves Lc≤Kc predictions further than threhold to false positives: FPc,≥20∘=Lc, and FPc=FPc,∞+FPc,≥20∘  \n",
    "the remaining matched estimates per class are counted as true positives: TPc,≤20∘=Kc−FPc,≥20∘  \n",
    "finally: predictions Pc=TPc,≤20∘+FPc, but references Rc=TPc,≤20∘+FPc,≥20∘+FNc  \n",
    "Based on those, we form the location-dependent F1-score (F≤20∘) and Error Rate (ER≤20∘). Contrary to the previous challenges, in which F≤20∘ was micro-averaged, in this challenge we perform macro-averaging of the location-dependent F1-score: F≤20∘=∑cFc,≤20∘/C.  \n",
    "  \n",
    "Additionally, we evaluate localization accuracy through a class-dependent localization error LEc, computed as the mean angular error of the matched true positives per class, and then macro-averaged:\n",
    "  \n",
    "LEc=∑kθk/Kc=∑kθk/TPc for each frame or segment, with θk being the angular error between the kth matched prediction and re  ference,  \n",
    "and after averaging across all frames that have any true positives, LECD=∑cLEc/C.\n",
    "Complementary to the localization error, we compute a localization recall metric per class, also macro-averaged:  \n",
    "\n",
    "LRc=Kc/Rc=TPc/(TPc+FNc), and\n",
    "LRCD=∑cLRc/C.\n",
    "Note that the localization error and recall are not thresholded in order to give more varied complementary information to the location-dependent F1-score, presenting localization accuracy outside of the spatial threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ffa09c-59ff-4b70-9cf6-029b87ead06c",
   "metadata": {},
   "source": [
    "# Ranking\n",
    "Overall ranking will be based on the cumulative rank of the metrics mentioned above, sorted in ascending order. By cumulative rank we mean the following: if system A was ranked individually for each metric as ER:1,F1:1,LE:3,LR:1, then its cumulative rank is 1+1+3+1=6. Then if system B has ER:3,F1:2,LE:2,LR:3 (10), and system C has ER:2,F1:3,LE:1,LR:2 (8), then the overall rank of the systems is A,C,B. If two systems end up with the same cumulative rank, then they are assumed to have equal place in the challenge, even though they will be listed alphabetically in the ranking tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "7adc501d-b120-4dcf-9e53-9e184cbd3d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_eval = \"/home/data/kbh/DCASE_eval/\"\n",
    "\n",
    "dir_label = \"eval_label\"\n",
    "#dir_sub = \"submission-665\"\n",
    "dir_sub = \"3track-submission652\"\n",
    "dir_audio = \"foa_eval\"\n",
    "\n",
    "list_label = [x for x in glob.glob(os.path.join(root_eval,dir_label,\"*.csv\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "6756ef79-0b5e-48fb-9cf7-e223b514335d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(path_target):\n",
    "    name_target = path_target.split('/')[-1]\n",
    "    id_target = name_target.split('.')[0]\n",
    "    \n",
    "    path_sub = os.path.join(root_eval,dir_sub,name_target)\n",
    "    path_aud = os.path.join(root_eval,dir_audio,id_target+\".wav\")\n",
    "    \n",
    "    raw,_ = librosa.load(path_aud,sr=24000)\n",
    "    len_target = len(raw)\n",
    "    n_label = int(len_target/24000)\n",
    "    \n",
    "    #print(\"Target :  {}\".format(id_target))\n",
    "    #print(n_label)\n",
    "    \n",
    "    # Label\n",
    "    csv_label = pd.read_csv(\n",
    "        path_target,\n",
    "        names=[\"idx\",\"1\",\"2\",\"3\",\"4\",\"5\"],\n",
    "        index_col=\"idx\",\n",
    "        #dtype=np.int32,\n",
    "        keep_default_na=False,\n",
    "    )\n",
    "    label = csv_label[:n_label]\n",
    "    \n",
    "    csv_sub   = pd.read_csv(path_sub)\n",
    "    # submission compression\n",
    "    \n",
    "    sub = pd.DataFrame(np.empty((n_label, 5),dtype=str) ,columns = [\"1\",\"2\",\"3\",\"4\",\"5\"])\n",
    "    \n",
    "    #display(csv_sub)\n",
    "    for i_sub in csv_sub.index:\n",
    "        idx = csv_sub.loc[i_sub][0]\n",
    "        cat = csv_sub.loc[i_sub][1]\n",
    "        \n",
    "        j_sub = int(idx/10)\n",
    "        \n",
    "        if j_sub >= n_label:\n",
    "            break\n",
    "        \n",
    "        # print(\"{} {} {}\".format(idx,j_sub,cat))\n",
    "        i_src = 0\n",
    "        \n",
    "        inserted = False\n",
    "        for i_src in range(5) : \n",
    "            if sub.iloc[j_sub][i_src] == \"\" :\n",
    "                sub.iloc[j_sub][i_src] = cat\n",
    "                inserted = True\n",
    "                break\n",
    "            # dup\n",
    "            elif sub.iloc[j_sub][i_src] == cat:\n",
    "                inserted = True\n",
    "                break\n",
    "                \n",
    "        if not inserted :\n",
    "            print(\"{} sub[{}] more than 5 soruce\".format(id_target,j_sub))\n",
    "    \n",
    "    \n",
    "    ## Eval\n",
    "    \n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    TN = 0\n",
    "    \n",
    "    for i_label in range(n_label) : \n",
    "        s_label = label.iloc[i_label]\n",
    "        s_sub   = sub.iloc[i_label]\n",
    "        \n",
    "        l_label = list(filter(None, label.iloc[i_label]))\n",
    "        l_sub = list(filter(None, sub.iloc[i_label]))\n",
    "        \n",
    "        # Exception\n",
    "        \n",
    "        if \" \" in l_label : \n",
    "            l_label = l_label.remove(\" \")\n",
    "            if l_label is None : \n",
    "                l_label = []\n",
    "        \n",
    "        if len(l_label) == 0 and len(l_sub) == 0 :\n",
    "            TN +=1\n",
    "            continue\n",
    "            \n",
    "        l_label = list(map(int, l_label))\n",
    "\n",
    "        #print(\"=====\")\n",
    "        #print(\"label : {}\".format(l_label))\n",
    "        #print(\"submt : {}\".format(l_sub))\n",
    "        \n",
    "        ## True\n",
    "        for iter_sub in l_sub : \n",
    "            if iter_sub in l_label : \n",
    "                TP+=1\n",
    "            else : \n",
    "                FP+=1\n",
    "                \n",
    "        for iter_label in l_label :\n",
    "            if iter_label not in l_sub:\n",
    "                FN+=1\n",
    "\n",
    "    recall = TP/(TP+FN)\n",
    "    f1 = (2*TP)/(2*TP+FP+FN)\n",
    "    acc = (TP+TN)/(TP+TN+FP+FN)\n",
    "                \n",
    "    \"\"\"       \n",
    "    print(\"    |  PP   |   PN \")\n",
    "    print(\" P  | {:2d}    |   {:2d}\".format(TP,FN))\n",
    "    print(\" N  | {:2d}    |   {:2d}\".format(FP, TN))\n",
    "    print(\"f1-score {:.3f}\".format(f1))\n",
    "    print(\"accuracy {:.3f}\".format(acc))\n",
    "    print(\"recall   {:.3f}\".format(recall))\n",
    "    \"\"\"\n",
    "    \n",
    "    return recall,f1,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "8b214aee-0048-4567-b587-aa91042e94e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 29/29 [00:08<00:00,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3track-submission652\n",
      "n_target : 29\n",
      "f1-score 0.625\n",
      "accuracy 0.542\n",
      "recall   0.587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "list_label = [x for x in glob.glob(os.path.join(root_eval,dir_label,\"*.csv\"))]\n",
    "\n",
    "recall = 0.0\n",
    "f1 = 0.0\n",
    "acc = 0.0\n",
    "\n",
    "#print(eval(\"/home/data/kbh/DCASE_eval/eval_label/mix025.csv\"))\n",
    "\n",
    "\n",
    "for path in tqdm(list_label) : \n",
    "    t_recall, t_f1, t_acc = eval(path)\n",
    "    recall += t_recall\n",
    "    f1 += t_f1\n",
    "    acc += t_acc\n",
    "\n",
    "        \n",
    "n_target = len(list_label)\n",
    "\n",
    "print(dir_sub)\n",
    "print(\"n_target : {}\".format(n_target))\n",
    "    \n",
    "print(\"f1-score {:.3f}\".format(f1/n_target))\n",
    "print(\"accuracy {:.3f}\".format(acc/n_target))\n",
    "print(\"recall   {:.3f}\".format(recall/n_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d27b7de-96ac-41ba-8a3f-d3d3330b1088",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
